---
TODO: 
- rewatch lesson
---
## Overfitting and underfitting
- __overfitting__: the model is too complex and specialized over the peculiarities of the samples in the training set 
- __underfitting__: the model is too simple and does not allow to express the complexity of the observations. 

>[!remark]
> Deep models are good at fitting, but the real goal is generalization

![[overfitting.png]]

## Ways to reduce overfitting 
- __Collect more data:__
	- the more the test data, the most likely the predictions will be similar to real data -> the more likely you can capture the variability of the real world. 
- __Reduce the model capacity__
	- \[\]
- __Early stopping__:
	- If the results are not improving, I simply stop
	- You can use callbacks to save weights at the end of this epoch (i.e. ``)
- __Regularization__, e.g. Weight-decay
	- $l_2$ regularization, you penalize the weights of the model to keep them close to 0. 
	- We try to keep the weights small. 
	- Weight-decay: the weights decay towards 0.
	- In each layer you add regularizers, and you can choose the kind of regularizers that you want. 
- __Model averaging__ 
	- i.e. in Random forests, we compute an average of the results obtained from each tree. 
- __Data augmentation__
	- We add data that we presume that is consistent with the real data.
	- Can have dangerous repercussions. 
- __Dropout__

## Dropout
Idea: “_cripple_” the neural network stocastically removing hidden units 
- during training, at each iteration _hidden units are disabled_ with probability $p$ (e.g. 0.5) 
- hidden units cannot co-adapts with other units 
- This makes your network much more robust. 
	- You try to train in parallel many different models trying to solve many different tasks. 
- similar to train many networks and averaging between them

Another way to make the model more robust is to inject more nodes, and check the results with very noisy data (i.e. adding salt and pepper noise). 

# Activation and loss functions for classification

## Sigmoid
When the result of the network is a value between 0 and 1, e.g. a probability for a binary classification problem, it is customary to use the _sigmoid function_:
$$
\sigma(x) = \dfrac{1}{1+e^{-x}} = \dfrac{e^x}{1+e^x}
$$
as activation function. 
If 
$$
P(Y=1|x) = \sigma(f(x)) = \dfrac{e^{f(x)}}{1+e^{f(x)}}
$$
then
$$
P(Y=0|x) = 1 - \sigma(f(x)) = \dfrac{1}{1+e^{f(x)}}
$$
A probability distribution of the input wrt to all the categories of the output that we are interested in. 

## Softmax
When the result of the network is a probability distribution, e.g. over K different categories, the softmax function is used as activation:
$$
\text{softmax}(j,x_1, ..., x_k) = \dfrac{e^{x_j}}{\sum_{j=1}^k e^{x_j}}
$$
It is easy to see that 
$$
0 < \text{softmax}(j, x_1, ..., x_k) < 1
$$
and most importantly 
$$
\sum_{j=1}^k \text{softmax}(j, x_1, ..., x_k)
$$
since we expect probablilites to sum up 1.

## Softmax vs Sigmoid
\[...\]

## Loss functions
The output computed, usually, is a probability distribution. 
- More specifically, a categorical distribution, since the 