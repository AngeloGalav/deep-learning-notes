## Inception V3

\[...\]

The convolutional part is a long composition of inception modules. 
The final part is usually not very deep.

## Residual Learning
![[residual_learning.png]]
Instead of learning a function $F(x)$ you try to learn $F(x) + x$.

The advantage of this approach is to

![[res_learning_2.png]]

You add a residual computation to your input. You compute a kind of delta (which is F(x)) to the input, it is a kind of residual that can help in the computation. 

You add a residual shortcut connection every 2-3 layers.
Inception Resnet is an example of a such an architecture.

The main advantage is that along these \[...\] you have a backpropagation of the loss. 
There's no degradation of the gradient. 

#### Why Residual Learning works?
Not well understood yet. The usual explanation is that during back propagation, the gradient at higher layers can easily pass to lower layers, withouth being mediated by the weight layers, which may cause vanishing gradient or exploding gradient problem.

#### Residual Learning - Sum or concatenation?
The “sum” operation can be interpreted in a liberal way. A common variant consists in concatenating instead of adding (usually along the channel axis):
![[sum-or-concatenation.png]]

## Efficient Net
A decision that we have to make when developing image processing network (CNN) is, for example, the size of the input. 

ConvNets essentially grow in three directions:
- Layers: the number of layers 
- Channels: the number of channels for layers 
- Resolution: the spatial width of layers 
Is there a principled method to scale up ConvNets that can achieve better accuracy and efficiency?

\[...\]

## Transfer Learning
We try to transfer knowledge from a network to another, usually using initially a very good network that has been trained on a lot of data, and we try to transfer knowledge to a smaller net. 

In particular, on the resulting network, we just alter the layers that we are interested and delete the others, then add a couple of dense layers, and on those we'll do the _actual learning_.  

Transferring knowledge from problem A to problem B makes sense if:
- the two problems have _“similar” inputs_ 
- we have _much more training data for A_ than for B

In all layers, you typically have a trainable parameter (which is a boolean), and if it false the learning on this layer freezes.

![[transfer_learning_cnns.png]]

Finetuning is always dangerous, and there's a risk of overfitting and degrading (possibly) good information.

##### Expectations of transfer learning
![[what_we_expect.png]]

# Backpropagation for CNNs

How to change the weights of the kernel during backpropagation?
![[convolution_matrix.png]]
Each column corresponds to a different application of the kernel. 
$w_{i,j}$ is a kernel weight, with i and j being the row and column of the kernel respectively.
The matrix hass been transposed for convenience.


# Transposed convolutions
Normal convolutions with non unitarian strides downsample the input dimension.
In some cases, we may be interested to upsample the input, e.g. for 
- image to image processing, to obtain an image of the same dimension of the input (or higher) after some compression to an internal encoding 
- project feature maps to a higher-dimensional space
We shall see several applications later on.