## Inception V3

In inception-like CNNs, we have a deep/convolutional part comprised of inception modules, in which we extract features and information from the input. 
At the very end, we have some kind of pooling operation, and we process the information depending of which kind of purpose has our CNN (i.e., classification, regression...). 
This final part is usually not very, since it is comprised of only 2/3 dense layers. 

## ResNet - Residual Learning
This type of CNN is also used for image processing, and makes use of _Residual Learning_, which is described by this image:
![[residual_learning.png]]
In Residual Learning, we have some kind of connection from the input to the output. In particular, instead of learning a function $F(x)$ you try to learn $F(x) + x$.

The intuition of this approach may be better understood by this image:
![[res_learning_2.png]]
Essentially, we are adding a _residual computation_ to our input, so we are computing a kind of _delta_ (which is F(x)) to the input, it is a kind of residual that can help in the computation. 
In general, it can be seen as a way to see if our input has some kind of improvement or not. 

You add a residual shortcut connection every 2-3 layers.
Inception Resnet is an example of a such an architecture.
![[residual_example.png]]
The __main advantage__ is that along these links, you have a _better backpropagation_ of the _loss_. It's true that if use RELU (Rectified Linear Units) instead of, say a sigmoid, you do not have the problem of vanishing gradient, but in any case there's a quick degradation of the loss going deeper and deeper in the net. 
Instead, using this method, ==there's no _degradation of the gradient_==. 

#### Why Residual Learning works?
Not well understood yet. The usual explanation is that during back propagation, _the gradient at higher layers can easily pass to lower layers_, withouth being mediated by the weight layers, which may cause vanishing gradient or exploding gradient problem.

#### Residual Learning - Sum or concatenation?
The “sum” operation can be interpreted in a liberal way. A common variant consists in concatenating (_skip connection_) instead of adding (usually along the channel axis):
![[sum-or-concatenation.png]]
By using a concatenation, we are usually just skipping some parts of the network. 

[16:22]

## Efficient Net
A decision that we have to make when developing image processing network (CNN) is, for example, the size of the input. 

ConvNets essentially grow in three directions:
- Layers: the number of layers 
- Channels: the number of channels for layers 
- Resolution: the spatial width of layers 
Is there a principled method to scale up ConvNets that can achieve better accuracy and efficiency?

\[...\]

## Transfer Learning
We try to transfer knowledge from a network to another, usually using initially a very good network that has been trained on a lot of data, and we try to transfer knowledge to a smaller net. 

In particular, on the resulting network, we just alter the layers that we are interested and delete the others, then add a couple of dense layers, and on those we'll do the _actual learning_.  

Transferring knowledge from problem A to problem B makes sense if:
- the two problems have _“similar” inputs_ 
- we have _much more training data for A_ than for B

In all layers, you typically have a trainable parameter (which is a boolean), and if it false the learning on this layer freezes.

![[transfer_learning_cnns.png]]

Finetuning is always dangerous, and there's a risk of overfitting and degrading (possibly) good information.

##### Expectations of transfer learning
![[what_we_expect.png]]

# Backpropagation for CNNs

How to change the weights of the kernel during backpropagation?
![[convolution_matrix.png]]
Each column corresponds to a different application of the kernel. 
$w_{i,j}$ is a kernel weight, with i and j being the row and column of the kernel respectively.
The matrix hass been transposed for convenience.


# Transposed convolutions
Normal convolutions with non unitarian strides downsample the input dimension.
In some cases, we may be interested to upsample the input, e.g. for 
- image to image processing, to obtain an image of the same dimension of the input (or higher) after some compression to an internal encoding 
- project feature maps to a higher-dimensional space
We shall see several applications later on.