Each neuron in a neural network gets activated by specific patterns in the input image, defined by the weights in it receptive field.

The intuition is that neurons at higher layers should recognize increasingly complex patterns, obtained as a combination of previous patterns, over a larger receptive field. 

In the highest layers, neurons may start _recognizing patterns_ similar to _features of objects_ in the dataset, such as feathers, eyes, etc. In the final layers, neurons gets activated by “patterns” identifying objects in the category. 

##### Can we confirm such a claim?

## Visualization of hidden layers
Our goal: find a way to _visualize the kind of patterns a specific neuron gets activated by_.
![[visualize.png]]
The loss function $L(θ, x)$ of a NN depends on the _parameters_ $θ$ and the _input_ $x$. 
During training, we fix $x$ and compute the partial derivative of $L(θ, x)$ w.r.t the parameters $θ$ to adjust them in order to decrease the loss.
In the same way, _we can fix_ $θ$ and use partial derivatives w.r.t. input pixels in order to syntehsize images _minimizing the loss_. 
In this way, we can compute an activation of some neuron, and understand how I should modify my input to increase or decrease the activation of a neuron. Thus, we could find, for example, which kind of _input maximizes this kind of activation. 

## The gradient ascent technique
Start with a random image, e.g.
![[noise.png]]
- do _a forward pass_ using this image $x$ as input to the network to _compute the activation_ $a_i(x)$ caused by $x$ at some neuron (or at a whole layer) 
- do _a backward pass_ to compute the gradient of $\dfrac{∂ai(x)}{∂x}$ of $a_i(x)$ with respect to each pixel of the input image (this is the actual gradient ascent step). 
- _modify the image_ adding a small percentage of the gradient $\dfrac{∂ai(x)}{∂x}$ and repeat the process until we get _a sufficiently high activation of the neuron_.

> [!WARNING]
> There's no real difference between the gradient ascent and gradient descent process, since we can convert a minimization problem into a maximization problem by just negating the objective function.
> It's called gradient ascent only because we are trying to _increase_ a value (which is the value of the activation of the neuron). 

#### Example of visualization
![[visualization_example.png]]

## A different approach
A different approach would be in using an input image and trying to understand which parts of the image are actually recognized by the network. 
To do that, we take the image, we take a particular internal layer, and what we do is trying to minize the loss between the original image and the internal representation of the image that we are interested in. In this way, we are basically _synthesizing an image_ _that is not distinguishable_ from the original image in that specific layer of the network (meaning, they would produce the same activation). 

Essentially, we are trying to understand the inner representation at some layer by generating an image indistinguishable from the original one.

### The technique
- Goal: given an input image $x_0$ with an internal representation $Θ_0 = Θ(x_0)$, generate a different image $x$ such that $Θ(x) = Θ_0$, 
- Approach: via gradient ascent starting form a noise image. _Instead_ of optimizing towards a given category or the _activation_ of a neuron, _minimize the distance_ from $Θ_0$:
![[formula_gradient_ascent.png]]

Obviously, it is much simpler to minimize this function when we are at a layer that is close to the start of the network, since this is when the result is much more similar to the starting image.
- The more we traverse the network, the more the input becomes deconstructed and so it is more difficult to reconstruct. 


#### Results
![[results_ga.png]]
As we can see, the input becomes progressively fuzzier, and it seems that our network almost deconstructs the whole image. 

#### Inceptionism
![[inceptionism.png]]
you've seen this shit for sure in some creepy youtube video. 
Essentially, it is image manipulation that creates 